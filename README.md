# Hybrid Data Engineering + Data Science Learning Path
### Moving away from Datacamp's approach, and into something more unique to me.

--- 
> _**My prompt to GenAI:** Provide me with a learning journey which is self-directed and structured to explore various disciplines within data - from engineering to science. It should be designed to build my confidence, fluency, and hands-on capability through a practical, project-based approach without being tied to industry pressure or trends. Build off my existing understanding of Python and SQL, alongside referencing from text books I own._

---

## ğŸ§­ Learning Journey Structure 
| Module | Focus Area                          |
|--------|-------------------------------------|
| 1      | Data Engineering & Foundations      |
| 2      | Pipelines & Orchestration           |
| 3      | Analytics Engineering & Warehousing |
| 4      | Core Data Science & ML Principles   |
| 5      | Deployment, DevOps & Capstone       |

---

## ğŸ“ Initial Repo Structure
```
hybrid-data-engineering-datascience/
â”œâ”€â”€ Module_1_Foundations/
â”‚   â”œâ”€â”€ notes.md
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ exercises/
â”‚   â””â”€â”€ project/
â”‚       â”œâ”€â”€ README.md
â”‚       â””â”€â”€ csv_to_parquet_to_s3.py
â”œâ”€â”€ Module_2_Pipelines/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ airflow_etl_dag.py
â”‚   â”œâ”€â”€ tests/
â”‚   â””â”€â”€ logs/
â”œâ”€â”€ Module_3_Analytics_Eng/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ dbt_project/
â”‚   â””â”€â”€ bigquery_setup.md
â”œâ”€â”€ Module_4_DataScience_ML/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ core_concepts_notes.md
â”‚   â”œâ”€â”€ from_scratch_implementations/
â”‚   â”‚   â”œâ”€â”€ linear_regression.py
â”‚   â”‚   â”œâ”€â”€ k_nearest_neighbors.py
â”‚   â”‚   â””â”€â”€ decision_trees.py
â”‚   â””â”€â”€ ml_practice_notebooks/
â”œâ”€â”€ Module_5_Deployment/
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ dockerfile
â”‚   â”œâ”€â”€ deploy_instructions.md
â”‚   â””â”€â”€ README_project.md
â”œâ”€â”€ capstone/
â”‚   â”œâ”€â”€ risk_model_pipeline/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â””â”€â”€ dashboard_project/
â”‚       â””â”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â””â”€â”€ README.md
```


## ğŸ“¦ Module 1: Data Engineering & Foundations

### ğŸ§  Overview
This module introduces core data handling and engineering concepts, including tabular data wrangling, SQL querying, file format optimisation, and early exposure to cloud tools. It sets the technical base for downstream analytics and ML work.

### ğŸ“š References
- *Effective Pandas* â€” Matt Harrison (Chapters on cleaning, reshaping)
- *Data Science from Scratch* â€” Joel Grus (Data representation, stats intro)
- *Fundamentals of Data Engineering* â€” Joe Reis & Matt Housley (Ch. 1â€“2 on foundations and data lifecycle)
- Pandas docs: https://pandas.pydata.org/docs/
- AWS S3 documentation: https://docs.aws.amazon.com/s3/

### ğŸ§ª Tasks & Exercises
- [ ] Load a CSV into a Pandas DataFrame
- [ ] Identify and handle missing values
- [ ] Group data and perform aggregations
- [ ] Convert cleaned data to Parquet
- [ ] Write a Python script to upload to AWS S3 using Boto3

### ğŸ“„ Project
**Title:** CSV â†’ Clean Parquet â†’ S3 Upload
**Deliverables:**
- `csv_to_parquet_to_s3.py`
- Documented steps in `README.md`
- Notes in `notes.md`

### ğŸ“ Progress Log (`notes.md`)
Use this to track:
- What youâ€™re learning
- Errors or blockers you encounter
- Questions or assumptions to revisit
- Snippets, tricks, and gotchas

---

## ğŸ” Module 2: Pipelines & Orchestration

### ğŸ§  Overview
This module focuses on automating the movement and transformation of data through pipelines. You'll learn the core principles of ETL/ELT, how to build reliable workflows using tools like Airflow or Prefect, and how to test and monitor these systems.

### ğŸ“š References
- *Designing Machine Learning Systems* â€” Chip Huyen (Pipelines and production systems)
- *Fundamentals of Data Engineering* â€” Joe Reis & Matt Housley (Ch. 3â€“5 on pipeline design, orchestration, and testing)
- Airflow documentation: https://airflow.apache.org/
- Prefect docs: https://docs.prefect.io/
- Real Python ETL examples: https://realpython.com/etl-pipeline-python/

### ğŸ§ª Tasks & Exercises
- [ ] Design a basic ETL flow in pseudocode (Extract â†’ Transform â†’ Load)
- [ ] Write an ETL function in Python that loads, processes, and saves a file
- [ ] Create a simple Airflow DAG with 2â€“3 dependent tasks
- [ ] Test transformation functions using pytest
- [ ] Add logging to a Python script with timestamps and status info

### ğŸ“„ Project
**Title:** Scheduled ETL DAG to Process and Upload Data
**Deliverables:**
- `airflow_etl_dag.py`
- `tests/` with at least one test script
- Logs demonstrating job execution
- Documentation in `README.md`

### ğŸ“ Progress Log (`notes.md`)
Use this to track:
- DAG concepts and how scheduling works
- Pain points with Airflow or Prefect setup
- Which pipeline architecture concepts made sense
- Insights from testing/logging experiments

---

## ğŸ§± Module 3: Analytics Engineering & Warehousing

### ğŸ§  Overview
This module focuses on the transformation layer between raw data and analysis-ready outputs. You'll use tools like dbt to build structured, reliable, and documented data models. Youâ€™ll also explore data warehouse platforms and optimisation strategies.

### ğŸ“š References
- dbt documentation: https://docs.getdbt.com/
- *Designing Machine Learning Systems* â€” Chip Huyen (Ch. 5 on data tooling)
- *Fundamentals of Data Engineering* â€” Joe Reis & Matt Housley (Ch. 6â€“7 on modeling and warehousing)
- BigQuery docs: https://cloud.google.com/bigquery/docs
- *Effective Pandas* â€” Matt Harrison (joins, reshaping for staging layers) (joins, reshaping for staging layers)

### ğŸ§ª Tasks & Exercises
- [ ] Create a dbt project and connect it to a sample warehouse (e.g. BigQuery)
- [ ] Build staging models to clean raw input tables
- [ ] Create intermediate and final models using `ref()` and `config`
- [ ] Write and compile documentation using dbt docs
- [ ] Implement table partitioning or incremental models

### ğŸ“„ Project
**Title:** Analytics-Ready Data Warehouse Model
**Deliverables:**
- A working dbt project with at least 3 models (staging â†’ core â†’ reporting)
- A warehouse structure visible in BigQuery/Snowflake/Redshift
- Screenshots of compiled dbt docs
- Notes and learnings in `notes.md`

### ğŸ“ Progress Log (`notes.md`)
Use this to track:
- How dbtâ€™s transformation layers work
- Naming conventions and testing strategies
- Warehouse setup steps and cloud config tips
- Notes on performance tuning, indexing, or costs

---

## ğŸ¤– Module 4: Core Data Science & ML Principles

### ğŸ§  Overview
This module focuses on foundational data science and machine learning concepts. Inspired by *Data Science from Scratch*, it guides us through building core ML models from the ground up, reinforcing our understanding of how they work. Weâ€™ll also contrast our implementations with scikit-learn equivalents to solidify applied fluency.

### ğŸ“š References
- *Data Science from Scratch* â€” Joel Grus (entire book)
- *Hands-On Machine Learning* â€” AurÃ©lien GÃ©ron (Chapters on model training/evaluation)
- scikit-learn documentation: https://scikit-learn.org/stable/user_guide.html
- Python Standard Library: https://docs.python.org/3/library/

### ğŸ§ª Tasks & Exercises
- [ ] Manually implement linear regression from scratch (using lists and functions)
- [ ] Build k-Nearest Neighbours classifier using basic Python structures
- [ ] Create a decision tree using recursion and dictionaries
- [ ] Load a real dataset and train the same models using scikit-learn
- [ ] Compare performance between scratch and sklearn versions using metrics

### ğŸ“„ Project
**Title:** Core ML Models: From Scratch to Application
**Deliverables:**
- Python scripts for each implementation: `linear_regression.py`, `k_nearest_neighbors.py`, `decision_trees.py`
- Notebooks comparing those with sklearn: `ml_practice_notebooks/`
- `README.md` with theory explanations and reflections

### ğŸ“ Progress Log (`core_concepts_notes.md`)
Use this to track:
- Concepts from each *Data Science from Scratch* chapter
- Implementation challenges
- Design decisions (e.g., recursive vs iterative logic)
- Accuracy gaps or performance differences

---

## âš™ï¸ Module 5: Deployment & Capstone

### ğŸ§  Overview
This module brings everything together â€” deploying your data pipelines or ML models into environments where others can access or interact with them. It introduces version control, containerisation, automation, and basic DevOps concepts to help you productionise and share your work.

### ğŸ“š References
- Docker documentation: https://docs.docker.com/
- GitHub Docs: https://docs.github.com/
- *Designing Machine Learning Systems* â€” Chip Huyen (Ch. 7â€“9 on deployment and monitoring)
- MkDocs: https://www.mkdocs.org/
- Streamlit: https://streamlit.io/

### ğŸ§ª Tasks & Exercises
- [ ] Dockerise a Python script or notebook
- [ ] Create a `requirements.txt` and Dockerfile
- [ ] Push code to GitHub and practice branching/merging
- [ ] Add logging to an ETL/ML pipeline
- [ ] Build a simple Streamlit or CLI dashboard from model output

### ğŸ“„ Project
**Title:** Deployable Data Product (Pipeline, ML Model, or Dashboard)
**Deliverables:**
- A working Docker image
- Deployed code pushed to GitHub
- `README_project.md` with install/run instructions
- If applicable, dashboard or notebook interface

### ğŸ“ Progress Log (`notes.md`)
Use this to track:
- Docker/CLI config or networking issues
- Git usage improvements and hiccups
- Deployment blockers or cloud lessons
- Notes on versioning, reproducibility, and feedback handling

---

